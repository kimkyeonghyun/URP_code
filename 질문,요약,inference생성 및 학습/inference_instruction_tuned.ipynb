{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d3080-953a-4772-b578-c9ec0a459d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>신청번호</th>\n",
       "      <th>신청일자</th>\n",
       "      <th>민원제목</th>\n",
       "      <th>민원요지</th>\n",
       "      <th>민원내용</th>\n",
       "      <th>처리결과</th>\n",
       "      <th>1차추가답변</th>\n",
       "      <th>1차추가답변일</th>\n",
       "      <th>2차추가답변</th>\n",
       "      <th>2차추가답변일</th>\n",
       "      <th>year</th>\n",
       "      <th>추출된 법</th>\n",
       "      <th>추출된 법(re)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9777</td>\n",
       "      <td>1AA-2308-0346918</td>\n",
       "      <td>2023-08-09 10:47</td>\n",
       "      <td>용도변경시 구조내력 허용 기준</td>\n",
       "      <td>구조 안전의 확인 등</td>\n",
       "      <td>\"건축물 내진설계기준 KDS 41 17 00\" 에 따르면 \"기존 부분에 대해서는 ...</td>\n",
       "      <td>평소 국토교통 행정에 관심과 애정을 가져주신 것에 감사드립니다.\\r\\n귀하께서 신청...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>Law: 건축법 제19조제7항, 건축법 시행령 제32조제2항, 건축구조기준(KDS ...</td>\n",
       "      <td>['건축법 제19조', '건축법 시행령 제32조']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index              신청번호              신청일자              민원제목         민원요지  \\\n",
       "0   9777  1AA-2308-0346918  2023-08-09 10:47  용도변경시 구조내력 허용 기준  구조 안전의 확인 등   \n",
       "\n",
       "                                                민원내용  \\\n",
       "0   \"건축물 내진설계기준 KDS 41 17 00\" 에 따르면 \"기존 부분에 대해서는 ...   \n",
       "\n",
       "                                                처리결과 1차추가답변 1차추가답변일 2차추가답변  \\\n",
       "0  평소 국토교통 행정에 관심과 애정을 가져주신 것에 감사드립니다.\\r\\n귀하께서 신청...    NaN     NaN    NaN   \n",
       "\n",
       "  2차추가답변일  year                                              추출된 법  \\\n",
       "0     NaN  2023  Law: 건축법 제19조제7항, 건축법 시행령 제32조제2항, 건축구조기준(KDS ...   \n",
       "\n",
       "                      추출된 법(re)  \n",
       "0  ['건축법 제19조', '건축법 시행령 제32조']  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "test_data = pd.read_csv('./materials/민원22~24년도통합_test_1010_law.csv', encoding='utf-8-sig')\n",
    "test_data.dropna(subset=['민원내용'], inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# preprocess texts\n",
    "def clean_cut_text(text):\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text) \n",
    "    text = re.sub(r'\\{[^}]*\\}', '', text)\n",
    "    text = re.sub(r'\\[[^]]*\\]', '', text) \n",
    "    text = re.sub(r'\\<[^>]*\\>', '', text)\n",
    "\n",
    "    text = re.sub(r'\\r\\n', ' ', text)\n",
    "    text = re.sub(r'[\\s]+', ' ', text)\n",
    "    return text.strip()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26916b-0de7-4b78-8e7e-6d33da7292c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# set available devices\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88947134-b07e-44b9-a704-6dd777a541f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/vllm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./EEVE-10.8b-sumsep-no12-1000/tokenizer_config.json',\n",
       " './EEVE-10.8b-sumsep-no12-1000/special_tokens_map.json',\n",
       " './EEVE-10.8b-sumsep-no12-1000/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\", cache_dir='./models')\n",
    "\n",
    "savename = 'EEVE-10.8b-sumsep-no12-1000'  # set a name for instruction-tuned model\n",
    "save_dir = f'./{savename}'  # set a direction for instruction-tuned model\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beab1d0-cbf1-45bb-a300-e300b31f4d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 02:39:16,592\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-05 02:39:22 config.py:905] Defaulting to use mp for distributed inference\n",
      "INFO 11-05 02:39:22 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='./EEVE-10.8b-sumsep-no12-1000', speculative_config=None, tokenizer='./EEVE-10.8b-sumsep-no12-1000', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./EEVE-10.8b-sumsep-no12-1000, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-05 02:39:22 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-05 02:39:22 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3064066)\u001b[0;0m INFO 11-05 02:39:23 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-05 02:39:27 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-05 02:39:27 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3064066)\u001b[0;0m INFO 11-05 02:39:27 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3064066)\u001b[0;0m INFO 11-05 02:39:27 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-05 02:39:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jovyan/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3064066)\u001b[0;0m INFO 11-05 02:39:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jovyan/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 11-05 02:39:33 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7253d0e03670>, local_subscribe_port=54537, remote_subscribe_port=None)\n",
      "INFO 11-05 02:39:33 model_runner.py:1056] Starting to load model ./EEVE-10.8b-sumsep-no12-1000...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3064066)\u001b[0;0m INFO 11-05 02:39:33 model_runner.py:1056] Starting to load model ./EEVE-10.8b-sumsep-no12-1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:01,  2.71it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.49it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.23it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:03<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3064066)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:04<00:00,  1.16it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:04<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-05 02:39:38 model_runner.py:1067] Loading model weights took 10.0642 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-05 02:39:39 model_runner.py:1067] Loading model weights took 10.0642 GB\n",
      "INFO 11-05 02:39:45 distributed_gpu_executor.py:57] # GPU blocks: 49002, # CPU blocks: 2730\n",
      "INFO 11-05 02:39:45 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 191.41x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3064066)\u001b[0;0m INFO 11-05 02:39:51 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3064066)\u001b[0;0m INFO 11-05 02:39:51 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-05 02:39:51 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-05 02:39:51 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-05 02:40:08 custom_all_reduce.py:233] Registering 3395 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3064066)\u001b[0;0m INFO 11-05 02:40:08 custom_all_reduce.py:233] Registering 3395 cuda graph addresses\n",
      "INFO 11-05 02:40:08 model_runner.py:1523] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3064066)\u001b[0;0m INFO 11-05 02:40:08 model_runner.py:1523] Graph capturing finished in 17 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "# load instruction-tuned model\n",
    "llm = LLM(model=save_dir, tensor_parallel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0cb627-a13e-4f91-91e6-294e0d7827f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6056/6056 [00:01<00:00, 5419.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_messages(minwon):\n",
    "    instruction = f\"민원: {minwon}\\n\\n해당 민원을 가장 잘 설명하는 법률 정보를 한국어로 생성하세요.\"\n",
    "    messages = [{'role': 'user', 'content': instruction}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "# make prompts for test data\n",
    "prompt_lst = []\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    cur_minwon = test_data.loc[i]['민원내용']\n",
    "    cur_minwon = clean_cut_text(cur_minwon)\n",
    "    message = get_messages(cur_minwon)\n",
    "    message = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "    prompt_lst.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249bc6b9-2b48-4972-bffc-581fc28160e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db21fcbe-42fc-4f13-85a9-d3666df711a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6056/6056 [15:11<00:00,  6.64it/s, est. speed input: 2078.80 toks/s, output: 6500.62 toks/s] \n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompt_lst, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca05d4-6b90-42c7-9cce-74984c4cb826",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [output.outputs[0].text for output in outputs]  # the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf7d72c-e5fa-4f0e-ac2c-7ae6780fdbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['infenrence'] = contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa725f7-3cd1-45fe-80c8-c3ccd0bccc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the inferenced results\n",
    "\n",
    "test_data.to_csv('./materials/민원22~24년도통합_test_1014_sepinference.csv', encoding='utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
